<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://iclr-blogposts.github.io/2025/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iclr-blogposts.github.io/2025/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-27T02:25:58+08:00</updated><id>https://iclr-blogposts.github.io/2025/feed.xml</id><title type="html">ICLR Blogposts 2025</title><subtitle>Home to the 2025 ICLR Blogposts track </subtitle><entry><title type="html">Sample Blog Post</title><link href="https://iclr-blogposts.github.io/2025/blog/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://iclr-blogposts.github.io/2025/blog/distill-example</id><content type="html" xml:base="https://iclr-blogposts.github.io/2025/blog/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-28-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %} 
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2025-04-28-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span> <span class="na">src=</span><span class="s">"{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}"</span> <span class="na">frameborder=</span><span class="s">'0'</span> <span class="na">scrolling=</span><span class="s">'no'</span> <span class="na">height=</span><span class="s">"600px"</span> <span class="na">width=</span><span class="s">"100%"</span><span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2025/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p> <p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p> <p><strong>Note:</strong> This is not supported for local rendering!</p> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1743013566052" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1743013566052 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1743013566052 .node circle,#mermaid-1743013566052 .node ellipse,#mermaid-1743013566052 .node polygon,#mermaid-1743013566052 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1743013566052 .node.clickable{cursor:pointer}#mermaid-1743013566052 .arrowheadPath{fill:#333}#mermaid-1743013566052 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1743013566052 .edgeLabel{background-color:#e8e8e8}#mermaid-1743013566052 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1743013566052 .cluster text{fill:#333}#mermaid-1743013566052 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1743013566052 .actor{stroke:#ccf;fill:#ececff}#mermaid-1743013566052 text.actor{fill:#000;stroke:none}#mermaid-1743013566052 .actor-line{stroke:grey}#mermaid-1743013566052 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1743013566052 .messageLine0,#mermaid-1743013566052 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1743013566052 #arrowhead{fill:#333}#mermaid-1743013566052 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1743013566052 .messageText{fill:#333;stroke:none}#mermaid-1743013566052 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1743013566052 .labelText,#mermaid-1743013566052 .loopText{fill:#000;stroke:none}#mermaid-1743013566052 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1743013566052 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1743013566052 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1743013566052 .section{stroke:none;opacity:.2}#mermaid-1743013566052 .section0{fill:rgba(102,102,255,.49)}#mermaid-1743013566052 .section2{fill:#fff400}#mermaid-1743013566052 .section1,#mermaid-1743013566052 .section3{fill:#fff;opacity:.2}#mermaid-1743013566052 .sectionTitle0,#mermaid-1743013566052 .sectionTitle1,#mermaid-1743013566052 .sectionTitle2,#mermaid-1743013566052 .sectionTitle3{fill:#333}#mermaid-1743013566052 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1743013566052 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1743013566052 .grid path{stroke-width:0}#mermaid-1743013566052 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1743013566052 .task{stroke-width:2}#mermaid-1743013566052 .taskText{text-anchor:middle;font-size:11px}#mermaid-1743013566052 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1743013566052 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1743013566052 .taskText0,#mermaid-1743013566052 .taskText1,#mermaid-1743013566052 .taskText2,#mermaid-1743013566052 .taskText3{fill:#fff}#mermaid-1743013566052 .task0,#mermaid-1743013566052 .task1,#mermaid-1743013566052 .task2,#mermaid-1743013566052 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1743013566052 .taskTextOutside0,#mermaid-1743013566052 .taskTextOutside1,#mermaid-1743013566052 .taskTextOutside2,#mermaid-1743013566052 .taskTextOutside3{fill:#000}#mermaid-1743013566052 .active0,#mermaid-1743013566052 .active1,#mermaid-1743013566052 .active2,#mermaid-1743013566052 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1743013566052 .activeText0,#mermaid-1743013566052 .activeText1,#mermaid-1743013566052 .activeText2,#mermaid-1743013566052 .activeText3{fill:#000!important}#mermaid-1743013566052 .done0,#mermaid-1743013566052 .done1,#mermaid-1743013566052 .done2,#mermaid-1743013566052 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1743013566052 .doneText0,#mermaid-1743013566052 .doneText1,#mermaid-1743013566052 .doneText2,#mermaid-1743013566052 .doneText3{fill:#000!important}#mermaid-1743013566052 .crit0,#mermaid-1743013566052 .crit1,#mermaid-1743013566052 .crit2,#mermaid-1743013566052 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1743013566052 .activeCrit0,#mermaid-1743013566052 .activeCrit1,#mermaid-1743013566052 .activeCrit2,#mermaid-1743013566052 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1743013566052 .doneCrit0,#mermaid-1743013566052 .doneCrit1,#mermaid-1743013566052 .doneCrit2,#mermaid-1743013566052 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1743013566052 .activeCritText0,#mermaid-1743013566052 .activeCritText1,#mermaid-1743013566052 .activeCritText2,#mermaid-1743013566052 .activeCritText3,#mermaid-1743013566052 .doneCritText0,#mermaid-1743013566052 .doneCritText1,#mermaid-1743013566052 .doneCritText2,#mermaid-1743013566052 .doneCritText3{fill:#000!important}#mermaid-1743013566052 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1743013566052 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1743013566052 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1743013566052 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1743013566052 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1743013566052 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1743013566052 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1743013566052 #compositionEnd,#mermaid-1743013566052 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1743013566052 #aggregationEnd,#mermaid-1743013566052 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1743013566052 #dependencyEnd,#mermaid-1743013566052 #dependencyStart,#mermaid-1743013566052 #extensionEnd,#mermaid-1743013566052 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1743013566052 .branch-label,#mermaid-1743013566052 .commit-id,#mermaid-1743013566052 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1743013566052{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> <ul> <li>Unordered lists can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Sample Blog Post (HTML version)</title><link href="https://iclr-blogposts.github.io/2025/blog/distill-example2/" rel="alternate" type="text/html" title="Sample Blog Post (HTML version)"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://iclr-blogposts.github.io/2025/blog/distill-example2</id><content type="html" xml:base="https://iclr-blogposts.github.io/2025/blog/distill-example2/"><![CDATA[<p> This is a sample blog post written in HTML (while the other <a href="/2025/blog/distill-example/">sample post</a> is written in Markdown). Authors have the choice to write in HTML or Markdown. While Markdown is easier to write, HTML gives you more control over the layout of your post. Furthermore, Markdown often interacts in unexpected ways with MathJax and other HTML widgets. If you are having trouble with Markdown, try writing in HTML instead. </p> <p> Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling. </p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code>$$</code>, like <code>$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code>$$</code> and place it as a separate paragraph. Here is an example: $$ \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) $$ </p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. You can display images from this repository using the following code:</p> <pre><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}</code></pre> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p> To ensure that there are no namespace conflicts, you must save your asset to your unique directory `/assets/img/2025-04-28-[SUBMISSION NAME]` within your submission. </p> <p> Please avoid using the direct HTML method of embedding images; they may not be properly resized. Some below complex ways to load images (note the different styles of the shapes/shadows): </p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3>Interactive Figures</h3> <p> Here's how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work. All that's required is for you to export your figure into HTML format, and make sure that the file exists in the `assets/html/[SUBMISSION NAME]/` directory in this repository's root directory. To embed it into any page, simply insert the following code anywhere into your page. </p> <pre><code>{% include [FIGURE_NAME].html %}</code></pre> <p> For example, the following code can be used to generate the figure underneath it. </p> <pre><code class="language-python">import pandas as pd
import plotly.express as px

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')

fig = px.density_mapbox(
    df, lat='Latitude', lon='Longitude', z='Magnitude', radius=10,
    center=dict(lat=0, lon=180), zoom=0, mapbox_style="stamen-terrain")
fig.show()

fig.write_html('./assets/html/2025-04-28-distill-example/plotly_demo_1.html')
</code></pre> And then include it with the following: <pre><code class="language-html">&lt;div class="l-page"&gt;
  &lt;iframe src="{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}" frameborder='0' scrolling='no' height="600px" width="100%"&gt;&lt;/iframe&gt;
&lt;/div&gt;
</code></pre> Voila! <div class="l-page"> <iframe src="/2025/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder='0' scrolling='no' height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p> Citations are then used in the article body with the <code>&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas. </p> <p> The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it. </p> <p> Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well - the authors are human and it's nice for them to have the community associate them with their work. </p> <h2 id="footnotes">Footnotes</h2> <p> Just wrap the text you would like to show up in a footnote in a <code>&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote> </p> <h2 id="code-blocks">Code Blocks</h2> <p> This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag as follows: </p> <pre><code>
{% highlight c++ linenos %}  <br/> code code code <br/> {% endhighlight %}

</code></pre> The keyword `linenos` triggers display of line numbers. You can try toggling it on or off yourself below: <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <h2 id="diagrams">Diagrams</h2> <p> This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="http://mermaid.js.org/">mermaid</a>, <a href="https://plantuml.com/">plantuml</a>, <a href="https://vega.github.io/vega-lite/">vega-lite</a>, etc. </p> <p> <b>Note</b>different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to the <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> README. </p> <p> <b>Note:</b> This is not supported for local rendering! </p> <p> The diagram below was generated by the following code: </p> <pre><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice->>John: Hello John, how are you?
    John-->>Alice: Great!
{% endmermaid %}

</code></pre> <div class='jekyll-diagrams diagrams mermaid'> <svg id="mermaid-1743013566759" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1743013566759 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1743013566759 .node circle,#mermaid-1743013566759 .node ellipse,#mermaid-1743013566759 .node polygon,#mermaid-1743013566759 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1743013566759 .node.clickable{cursor:pointer}#mermaid-1743013566759 .arrowheadPath{fill:#333}#mermaid-1743013566759 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1743013566759 .edgeLabel{background-color:#e8e8e8}#mermaid-1743013566759 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1743013566759 .cluster text{fill:#333}#mermaid-1743013566759 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1743013566759 .actor{stroke:#ccf;fill:#ececff}#mermaid-1743013566759 text.actor{fill:#000;stroke:none}#mermaid-1743013566759 .actor-line{stroke:grey}#mermaid-1743013566759 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1743013566759 .messageLine0,#mermaid-1743013566759 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1743013566759 #arrowhead{fill:#333}#mermaid-1743013566759 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1743013566759 .messageText{fill:#333;stroke:none}#mermaid-1743013566759 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1743013566759 .labelText,#mermaid-1743013566759 .loopText{fill:#000;stroke:none}#mermaid-1743013566759 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1743013566759 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1743013566759 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1743013566759 .section{stroke:none;opacity:.2}#mermaid-1743013566759 .section0{fill:rgba(102,102,255,.49)}#mermaid-1743013566759 .section2{fill:#fff400}#mermaid-1743013566759 .section1,#mermaid-1743013566759 .section3{fill:#fff;opacity:.2}#mermaid-1743013566759 .sectionTitle0,#mermaid-1743013566759 .sectionTitle1,#mermaid-1743013566759 .sectionTitle2,#mermaid-1743013566759 .sectionTitle3{fill:#333}#mermaid-1743013566759 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1743013566759 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1743013566759 .grid path{stroke-width:0}#mermaid-1743013566759 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1743013566759 .task{stroke-width:2}#mermaid-1743013566759 .taskText{text-anchor:middle;font-size:11px}#mermaid-1743013566759 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1743013566759 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1743013566759 .taskText0,#mermaid-1743013566759 .taskText1,#mermaid-1743013566759 .taskText2,#mermaid-1743013566759 .taskText3{fill:#fff}#mermaid-1743013566759 .task0,#mermaid-1743013566759 .task1,#mermaid-1743013566759 .task2,#mermaid-1743013566759 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1743013566759 .taskTextOutside0,#mermaid-1743013566759 .taskTextOutside1,#mermaid-1743013566759 .taskTextOutside2,#mermaid-1743013566759 .taskTextOutside3{fill:#000}#mermaid-1743013566759 .active0,#mermaid-1743013566759 .active1,#mermaid-1743013566759 .active2,#mermaid-1743013566759 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1743013566759 .activeText0,#mermaid-1743013566759 .activeText1,#mermaid-1743013566759 .activeText2,#mermaid-1743013566759 .activeText3{fill:#000!important}#mermaid-1743013566759 .done0,#mermaid-1743013566759 .done1,#mermaid-1743013566759 .done2,#mermaid-1743013566759 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1743013566759 .doneText0,#mermaid-1743013566759 .doneText1,#mermaid-1743013566759 .doneText2,#mermaid-1743013566759 .doneText3{fill:#000!important}#mermaid-1743013566759 .crit0,#mermaid-1743013566759 .crit1,#mermaid-1743013566759 .crit2,#mermaid-1743013566759 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1743013566759 .activeCrit0,#mermaid-1743013566759 .activeCrit1,#mermaid-1743013566759 .activeCrit2,#mermaid-1743013566759 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1743013566759 .doneCrit0,#mermaid-1743013566759 .doneCrit1,#mermaid-1743013566759 .doneCrit2,#mermaid-1743013566759 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1743013566759 .activeCritText0,#mermaid-1743013566759 .activeCritText1,#mermaid-1743013566759 .activeCritText2,#mermaid-1743013566759 .activeCritText3,#mermaid-1743013566759 .doneCritText0,#mermaid-1743013566759 .doneCritText1,#mermaid-1743013566759 .doneCritText2,#mermaid-1743013566759 .doneCritText3{fill:#000!important}#mermaid-1743013566759 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1743013566759 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1743013566759 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1743013566759 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1743013566759 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1743013566759 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1743013566759 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1743013566759 #compositionEnd,#mermaid-1743013566759 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1743013566759 #aggregationEnd,#mermaid-1743013566759 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1743013566759 #dependencyEnd,#mermaid-1743013566759 #dependencyStart,#mermaid-1743013566759 #extensionEnd,#mermaid-1743013566759 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1743013566759 .branch-label,#mermaid-1743013566759 .commit-id,#mermaid-1743013566759 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1743013566759{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <h2 id="tweets">Tweets</h2> <p> An example of displaying a tweet: <div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> An example of pulling from a timeline: <div class='jekyll-twitter-plugin'><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a> </p> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <h2 id="layouts">Layouts</h2> The main text column is referred to as the body. It's the assumed layout of any direct descendants of the `d-article` element. <div class="fake-img l-body"> <p>.l-body</p> </div> For images you want to display a little larger, try `.l-page`: <div class="fake-img l-page"> <p>.l-page</p> </div> All of these have an outset variant if you want to poke out from the body text a little bit. For instance: <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> Occasionally you'll want to use the full browser width. For this, use `.l-screen`. You can also inset the element a little from the edge of the browser by using the inset variant. <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of `.l-body`-sized text except on mobile screen sizes. <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <h2 id="other-typography">Other Typography?</h2> <p> Emphasis, aka italics, with the <code>&lt;i&gt;&lt;/i&gt;</code> tag <i>emphasis</i>. </p> <p> Strong emphasis, aka bold, with <code>&lt;b&gt;&lt;/b&gt;</code> tag <b>bold</b>. </p> <p> Strikethrough ca be accomplished with the <code>&lt;s&gt;&lt;/s&gt;</code> tag. <s>Scratch this.</s> </p> <ul> <li>First ordered list item</li> <li>Another item</li> <ol> <li>Unordered sub-list. </li> </ol> <li>And another item.</li> </ul> <p> For code, the language can be specified in the class. For example, use <q>language-javascript</q> for Javascript and <q>language-python</q> for Python code. </p> <pre><code class="language-javascript">var s = "JavaScript syntax highlighting";
  alert(s);</code></pre> <pre><code class="language-python">s = "Python syntax highlighting"
  print(s)</code></pre> <pre><code class="language-python">No language indicated, so no syntax highlighting.</code></pre> <p> A table can be created with the <code>&lt;table&gt;</code> element. Below is an example </p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p> <blockquote>Blockquotes can be defined with the &gt;blockquote&lt; tag.</blockquote> </p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Do vision models perceive objects like toddlers ?</title><link href="https://iclr-blogposts.github.io/2025/blog/toddlers-vs-vismodels/" rel="alternate" type="text/html" title="Do vision models perceive objects like toddlers ?"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://iclr-blogposts.github.io/2025/blog/toddlers-vs-vismodels</id><content type="html" xml:base="https://iclr-blogposts.github.io/2025/blog/toddlers-vs-vismodels/"><![CDATA[<h1 id="introduction">Introduction</h1> <h2 id="motivation">Motivation</h2> <p>State-of-the-art machine learning vision models learn visual representation by learning from millions/billions of independently and identically distributed diverse images. In comparison, toddlers almost always play with/observe the same objects in the same playground/home/daycare environments. They likely have not seen 10% of the different platypuses or sea snakes on ImageNet. In sum, toddlers likely experience a diversity of objects which is several orders of magnitude lower than current models. Unlike current machine learning (ML) vision models, they also do not have access to a massive amount of category labels (or aligned language utterances) or adversarial samples. Despite using different learning mechanisms, they develop strong semantic representations that are robust to image distortions, viewpoints, machine-adversarial samples and different styles (drawings, silhouettes…) <d-cite key="wichmann2023deep, huber2023developmental"></d-cite>.</p> <p>What perceptual mechanisms underpin such a robust visual system ? Psychophysics experiments demonstrate that toddlers progressively reach fundamental visual milestones and develop visual biases during their second and third year. Specifically, toddlers become able to extract the category of simplified objects <d-cite key="smith2003learning"></d-cite> and learn to attend to the shapes of novel objects when judging their similarity based on their <em>kind</em> <d-cite key="diesendruck2003specific"></d-cite>. Simultaneously, they also manipulate object to clearly exhibit their main axis of elongation <d-cite key="pereira2014main"></d-cite>, and later, begin to semantically identify objects based on the configural arrangement between their parts <d-cite key="augustine2011parts"></d-cite>.</p> <p>In this blog post, we investigate the extent to which off-the-shelf ML models also exhibit these properties. Comparing vision models specifically to toddlers (not adults <d-cite key="bowers2023deep, wichmann2023deep"></d-cite>) allows us to identify which developmental milestones are not (yet) achieved by current vision models. We extend previous studies to give a global picture of the potential emergence of toddler-like visual properties in current pre-trained ML models. We include diverse models based on supervised learning, semi-weak supervision, robust out-of-distribution image recognition, self-supervised learning with instance discrimination (ID) and masked auto-encoders (MAE), adversarial training, vision-language models and egocentric visual training. We also try both convolutional architectures (CNN) and vision transformers (ViT). We refer to the end of the blog post for a complete list of models and why we chose them. Our objective is to 1) complement and extend prior experiments comparing ML to humans to (in)validate prior claims; 2) understand the (dis)similarities between the visual properties of ML models and toddlers; 3) clarify the interplay between different toddler-based visual properties in ML. We will also publicly release our code on github upon acceptance as an easy-to-use toolbox for assessing new models.</p> <h2 id="scope-of-the-study">Scope of the study</h2> <p>We divide our study into four parts, each corresponding to a specific visual property emerging in toddlers and remaining for the rest of their life. In the following, we describe each and clarify the scope of our experiments with respect to prior work. Note that we do not aim to faithfully reproduce evaluation protocols from developmental studies, as they are often based on language and we do not want to limit our experiments to vision-language models. Instead, we use alternatives that aim to assess the presence of a visual property relative to toddlers. Note that we still include vision-language models.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/overview-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/overview-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/overview-1400.webp"/> <img src="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/overview.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1. Illustration of emerging visual properties in toddlers. A: Caricature recognition. B: Shape bias. C: Preferred views. D: Sensitivity to spatial arrangement of features. Green rectangles in A, B indicate what toddlers have learned to judge as more similar. Blue rectangle in C indicates preference for side views. Red rectangle in D means that humans learn to consider the two stimuli with rearranged features as dissimilar. </div> <h4 id="caricature-recognition">Caricature recognition</h4> <p>At around 18 months, toddlers become able to recognize impoverished variants of daily-life objects called caricatures. These caricatures are constructed from few simple shapes <d-cite key="smith2003learning"></d-cite> (e.g. Figure 1, A. The recognition of these caricatures is related to the development of abstract representations and may boost category generalization <d-cite key="son2008simplicity"></d-cite>. To the best of our knowledge, the only study investigating ML models for caricature recognition focuses on vision-language models <d-cite key="sheybani2024modelvsbaby"></d-cite>. We modify their experimental protocol to compare a wider range of pre-trained models.</p> <h4 id="shape-bias">Shape bias</h4> <p>When asked to match objects based on their <em>kind</em> (but also names etc.), toddlers group together novel objects that share the same shape, rather than objects that share the same color or texture <d-cite key="diesendruck2003specific"></d-cite> (cf. Figure 1, B). This bias emerges slightly after learning caricature recognition <d-cite key="yee2012changes"></d-cite>. This effect has been widely investigated in ML models following the introduction of related datasets <d-cite key="geirhos2021partial, hermann2020origins"></d-cite>. However, mainstream evaluation protocols differ from evaluation protocols in the developmental literature <d-cite key="tartaglini2022developmentally"></d-cite>, which are known to be very sensitive to experimental details <d-cite key="frigo1998adjective"></d-cite>. Here, we extend the developmentally-relevant experiments of Tartaglini et al. (2022) to include more models, as they only evaluated 5 pre-trained models.</p> <h4 id="side-view-bias">Side-view bias</h4> <p>From 18-24 months onward, toddlers learn to manipulate objects to see them 1- in upright positions; 2- with their main axis of elongation perpendicular to the line of sight, which we call side view (e.g. Figure 1, C) <d-cite key="pereira2010early, pereira2014main"></d-cite>. Here, we focus on this side-view bias, which is a perceptual bias (not reflecting motor or grasping biases)<d-cite key="james2014some"></d-cite>. A more fine-grained look at toddlers’ behavior shows that they visually inspect these views while keeping the object almost static <d-cite key="pereira2014main,lisboa2014looking"></d-cite>. This suggests that these side-views are somehow perceptually <em>special</em>. However, the origins of this bias remain unclear. To the best of our knowledge, we are the first to investigate if these views are also special for ML models.</p> <h4 id="configural-relation-between-parts">Configural relation between parts</h4> <p>Starting at around 20 months, infants take into account the relative position of parts of an object to categorize it <d-cite key="augustine2011parts"></d-cite>. Such ability becomes fully mature for novel objects only later in development <d-cite key="mash2006multidimensional"></d-cite> and is a fundamental component of one of the main theories of human object recognition<d-cite key="hummel1992dynamic"></d-cite>. To the best of our knowledge, two lines of study have investigated this ability in ML models <d-cite key="baker2022deep,farahat2023novel"></d-cite> with two opposite conclusions. We discuss their differences and assumptions, extending the experiments of Baker et al. (2022) (Figure 1, D) to include more models.</p> <h2 id="tldr">TL;DR</h2> <p>Our analysis reveals two main visual milestones in machine learning models. The first one is the acquisition of the shape bias, which correlates with the ability to recognize normal objects and caricatures. This bias is close to that of toddlers in the strongest models. We also find that the shape bias is clearly connected to side views being better prototypes of object instances, <em>i.e.</em> they are relatively more similar to other views of an object. However, side-views are not prototypical enough to explain a side-view bias at the level of toddlers. The second one is the development of configural sensitivity, for which adults (and probably toddlers) level is out-of-reach for all investigated models. Neither of the two milestones are achieved by investigated models that were trained on a reasonable amount of visual egocentric data. In sum, comparing properties of toddlers object perception with those of artificial vision systems suggests that there is room for ML improvements in 1) making the development of the shape bias more data-efficient and 2) learning to generalize based on the configural arrangement of parts. For 1), one should not overlook CNN architectures, as we found them to show a more robust shape bias on novel shapes, compared to ViTs.</p> <h1 id="experiments">Experiments</h1> <h2 id="caricature-recognition-1">Caricature recognition</h2> <p>The main developmental protocol to evaluate caricature recognition is to ask a subject “where is the {object}” among a set of one object caricature matching the word and other non-matching object caricatures <d-cite key="smith2003learning"></d-cite>. This has been recently adapted to vision-language models by giving the object name to the language encoder and retrieving the closest image embeddings in a set of images <d-cite key="sheybani2024modelvsbaby"></d-cite>.</p> <p>Here, we modify their evaluation protocol to include models beyond vision-language models. We take the BabyVsModels dataset <d-cite key="sheybani2024modelvsbaby"></d-cite>, which contains 8 categories, each with 5 and 8 different images for realistic and caricatures, respectively. We create the set of all possible triplets, each constructed with a realistic image of an object, one caricature from the same category as the realistic image and one caricature from a different category. We compute their image representations and test whether the cosine similarity between the representation of the realistic image and its category-matching caricature is higher than the cosine similarity between the realistic image representation and the non-matching caricature. We define the recognition accuracy as the success rate across all triplets. We replicate the process with a harder 8-choices (versus two-choices) classification variant to check whether the difficulty of the task impacts our results. In this case, we use 100 tuples per realistic image. Furthermore, to estimate the recognition performance of models on normal images, we also compute the success rate on realistic images.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/caricaturesimg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/caricaturesimg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/caricaturesimg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/caricaturesimg.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 2. Illustration of how we compute A) simple caricature recognition accuracy and B) the normal image recognition accuracy. d is the negative cosine similarity. </div> <p>In Figure 3, A, we find that diverse models achieve a strong caricature recognition performance, at the estimated level of toddlers. Results confirm that supervised and vision-language models perform well on these tasks <d-cite key="sheybani2024modelvsbaby"></d-cite>. We notably find that ID models (DinoV2, BYOL, AA-SIMCLR) perform reasonably well on caricature recognition. Figure 3, B indicates that the hard caricature recognition accuracy correlates with hard image recognition accuracy (Pearson score: $0.78$). Overall, we conclude that strong caricature recognition may be a side-effect of strong category recognition and that cutting-edge models already reach the level of toddlers, independent of their exact training procedure.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/caricatures-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/caricatures-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/caricatures-1400.webp"/> <img src="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/caricatures.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3. A) Simple caricature and B) hard caricature accuracy against normal image accuracy recognition. We estimate 24-months toddler performance based on data from Pereira et al. (2009). This is consistent with results from Sheybani et al. (2024), which suggests that they perform similarly to pre-trained CLIP models. We refer to the "Models" section for more information about the groups of models. </div> <h2 id="shape-bias-1">Shape bias</h2> <p>It seems obvious that the best investigated models leverage objects’ shapes to measure image similarities, as texture and color cues are absent from caricatures. Here, we examine the shape bias of current models, compared to toddlers, and analyze the relationship between shape bias and caricature recognition.</p> <p>The cue-conflict protocol is the main experimental protocol to evaluate the shape bias in toddlers <d-cite key="diesendruck2003specific"></d-cite>. The idea is to present an object and two variants, one modifying only the texture of the original object and one modifying only the shape of the original object. The task is then to choose which of the two variants is the closest to the original one. If a toddler tends to select the same-shape variant, it means that they are shape biased.</p> <p>ML benchmarks used to evaluate the shape bias of models <d-cite key=" geirhos2021partial,hermann2020origins"></d-cite> do not follow the experimental protocols of developmental studies, as they texture the whole image instead of the object itself<d-cite key="smith2003learning"></d-cite>. This is important as it confounds using the shape and being able to segment the object. Thus we follow the cue-conflict experimental protocol of Tartaglini et al. (2022), who only texture the objects. First, we evaluate the shape bias on common (in-distribution) objects. We reuse the masks of common objects and 16 textures used in prior works <d-cite key="hermann2020origins,tartaglini2022developmentally"></d-cite>; this set includes 16 categories of objects, each containing 10 masks of different object instances. With these images, we build a set of triplets that follow the cue-conflict paradigm. In each triplet, we construct an anchor image mixing a shape with a texture, a variant built with the same shape but a different texture and another variant built with the same texture but a different shape. For each triplet, we define a shape bias success as the representation of the anchor being closer to the same shape variant than to the same texture variant. The shape bias accuracy is simply the success rate. The set of triplets includes all possible combinations. Second, to evaluate the shape bias on novel objects, we reproduce the same protocol using the 16 novel shapes from prior work <d-cite key="tartaglini2022developmentally, parks2020statistical"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/shapebiasimg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/shapebiasimg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/shapebiasimg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/shapebiasimg.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 4. Illustration of how we compute the degree of shape bias on A) common objects and B) novel shapes. $d$ is the negative cosine similarity. </div> <p>In Figure 5, A, we verify that there is a correlation between hard caricature recognition and the degree of shape bias over common objects (Pearson correlation: $0.68$). This is higher than the correlation with hard object recognition ($0. 56$). We also find that language is not a mandatory component of the shape bias as ID models seem to classify object based on shapes. In particular, DinoV2 shows a similar degree of shape bias as vision-language or supervised models. This is interesting because developmental studies clearly found that the productive vocabulary size is a strong predictor of the shape bias <d-cite key="gershkoff2004shape,jones2003late"></d-cite>. Our results suggest that one should not over-estimate the causal importance of language for the acquisition of this bias <d-cite key=" gavrikov2024vision,smith2002object"></d-cite>.</p> <p>In Tartaglini et al. (2022), the gap of shape bias between toddlers and models increases for novel shapes. When reproducing their results, we noticed that the performance of a random ResNet50 (Random_RN50, averaged over three random initializations) decreases between common and novel shapes. Since all shapes are novel for a random model, this suggests that the used novel shapes are inherently less prone to induce a shape-based matching than the common objects. To naively correct for this effect, we multiply all scores by the ratio \(\frac{\texttt{ShapeBiasCommon} (\texttt{Random_RN50})}{\texttt{ShapeBiasNovel}(\texttt{Random_RN50})}=1.334\) and show the results in Figure 5, B. We find that the strongest models perform close to toddlers’ performance; the best models, Noisy-Student and SWSL, match and outperforms toddlers performance, respectively. A thorough look at Figure 5, B seems to indicate that CNNs better generalize the shape bias to novel objects than ViTs (e.g. CLIP_RN50 versus CLIP_ViT-L/14), despite two outliers (Sup_RN50 and CLIP-LAION_CNX_XXL/32). This nuances previous claims of ViTs being more shape biased than CNNs<d-cite key="tuli2021convolutional,naseer2021intriguing"></d-cite>. Overall, the best ML models have a relatively similar level of shape bias to toddlers.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/shapebias-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/shapebias-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/shapebias-1400.webp"/> <img src="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/shapebias.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 5. Shape bias for A) common objects and B) novel objects. We extract 3-years old toddler performance from Diesendruck et al. (2003) <d-cite key="diesendruck2003specific"></d-cite> and assume the performance would be similar with our set of masks and textures. </div> <h2 id="side-views">Side views</h2> <p>The previous section indicates that ML models reach a high degree of shape bias with common objects. Simultaneously to its emergence, toddlers also focus their attention on side views of objects <d-cite key="pereira2010early,pereira2014main"></d-cite>, defined as views showing the main axis of elongation perpendicular to the line of sight (when the object is in a canonical upright position). To the best of our knowledge, the reasons for such a bias are currently unclear. A hypothesis may be that side views are particularly interesting for toddlers because they unambiguously display the whole shape, simplifying object recognition.</p> <p>To investigate this question, we leverage the OmniObject3D dataset <d-cite key=" wu2023omniobject3d"></d-cite> which contains 6,000 high-quality textured meshes scanned from real-world objects, distributed into 190 categories. We specifically use the provided recordings of 24 in-depth rotations per object in a canonical upright position. We preprocess this dataset by removing objects that do not show a clear main axis of elongation (e.g. a rotationally symmetric bottle). To do so, we remove objects for which the ratio between the narrowest and widest views on the horizontal axis is below $0.8$. The provided views were randomly sampled around the yaw axis, meaning that we do not access the exact side views of an object. Thus, we define the main side, main front, main 3/4 views of an object as the object’s views that have the closest angular rotation, with respect to a canonical side view, to $\{0, 180\}$, $\{-90, 90\}$, $\{-135, -45, 45, 135\}$ degrees, respectively.</p> <p>To assess the specificity of each of these main views, we propose two metrics:</p> <ul> <li>the intra-object distance as the average distance between the representation of a given main view of an object and the representations of all other views of the same object;</li> <li>the intra-category distance as the average distance between the representation of the given main view of an object and the representations of all other views in the same category.</li> </ul> <p>For each object, we retrieve the main view that maximizes and minimizes each metric. Finally, we compute the side-view accuracy as the average number of times the main side view maximizes these metrics. We similarly compute the 3/4-view and front-view accuracy. This allows us to assess which main orientation is more prototypical for an object or a category. Figure 6 illustrates the procedure.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/viewsimg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/viewsimg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/viewsimg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/viewsimg.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 6. Illustration of how we compute the side-view accuracy for intra-object distance. $d$ refers to the negative cosine similarity. </div> <p>First, we compute the Pearson correlation between the intra-object side-view accuracy and our previously reported measures. We find that the metric that most highly correlates with the side-view accuracy is the shape bias on common objects (Pearson correlation: $0.86$). The correlation is similar for intra-category side-view accuracy (Pearson correlation: $0. 80$). Visualizing the results in Figure 7 confirms that the higher the shape bias, the more prototypical the side views are. The fact that the shape bias correlates with side views being more prototypical aligns with the fact that the shape bias emerges during the same period as the side-view bias in toddlers. This correlation supports the hypothesis that toddlers may turn objects towards side views because they are more prototypical.</p> <p>Interestingly, AA-SimCLR <d-cite key="aubret2024self"></d-cite> notably presents a boost in side-view accuracy for a given level of shape bias. This model leverages egocentric actions that rotate objects during training and has been shown to better generalize visual representations in a viewpoint-wise fashion. This suggests that side-view accuracy can be improved in the best-performing models through such an approach.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/views-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/views-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/views-1400.webp"/> <img src="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/views.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 7. View accuracies for intra-object and intra-category similarity maximization. </div> <p>We did not find raw data on the time spent by toddlers on side views. However, current studies found (on a different set of objects) that toddlers focus more on these views than on 3/4 views <d-cite key="pereira2010early"></d-cite>. In our case, the 3/4 view accuracy remains beyond the reach for all models. Thus, if toddlers indeed target views because they are more prototypical, ML models show a far lower bias than toddlers.</p> <h2 id="configural-relation-between-parts-1">Configural relation between parts</h2> <p>Paying attention to the shape of an object is different from looking at the relative position of all parts. It could be that the model only extracts local <d-cite key="baker2020local"></d-cite> or intermediate <d-cite key="jarvers2023shape"></d-cite> shape features for recognizing categories. For instance, looking at the shape of the small ears of a bunny is often enough to recognize it. This may not be enough to unveil the potential of side views as these views also clearly display the configural arrangement of parts of an object. For instance, side-views exhibit that the head of an animal is usually located at the opposite side as its tail.</p> <p>The ability to categorize objects based on the configural arrangement of their parts starts its development in toddlers <d-cite key="augustine2011parts"></d-cite> and becomes mature only for older children <d-cite key="mash2006multidimensional"></d-cite>. Two lines of prior works studied whether ML models rely on the configural relation between parts to compare images <d-cite key="baker2022deep,farahat2023novel"></d-cite>. For both, the idea is the same: they try to modify the positions of parts of an image (e.g. a head, a tail, a wheel) without deteriorating the structure of these parts.</p> <p>In <d-cite key="farahat2023novel"></d-cite>, they train a CNN (with supervision) with a specific architecture that outputs representations corresponding to small parts of an image; then they spatially scramble these local representations and train (with supervision) another neural network on top of it. Their assumption is that the first model learns representations of local and coherent features: in that case, the scrambling process will mimic a change in relative feature positions. They found that the scrambling process hurts the recognition performance and conclude that current CNNs pay attention to the configural relation between parts. To the best of our knowledge, the validity of their assumption is unclear. In addition, their results could be specific to their CNN and the two-steps training process.</p> <p>In <d-cite key="baker2022deep,baker2018deep"></d-cite>, they propose to create <em>Frankenstein</em> silhouettes by taking an object silhouette and horizontally flipping the upper part of the silhouette, while keeping aligned the boundaries of the silhouette. The underlying assumption is that the flip operation does not alter the parts themselves. They found that the recognition performance of ML models is largely unaffected by the flipping process, unlike adults. We notice that they mostly use animals in their study, such that the lower and upper part of the silhouette contain distinct parts: the bottom part corresponds to the legs and the top part often displays the shape of the head (ears etc. ..).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/frankenstein-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/frankenstein-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/frankenstein-1400.webp"/> <img src="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/frankenstein.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Thus, we take the set of stimuli from Baker et al. (2022), which comprises 9 categories of animals, each containing 40 original and Frankenstein silhouettes. To reproduce experiments from Baker et al. (2022) without a classifier, we sample a normal silhouette from each category and one Frankenstein silhouette. We compute the cosine similarity between the Frankenstein silhouette and all normal silhouettes and define the accuracy as how often the cosine similarity between the Frankenstein and the category-matching normal silhouette is the largest (Frankenstein test). To assess the relative performance, we apply the procedure again after replacing the Frankenstein silhouette by another normal silhouette (normal test). Finally, we compute the configural sensitivity as the difference in performance between the success rate of the Frankenstein test and the success rate of the normal test. To show sensitivity to the spatial arrangement of parts, the metric must be lower than $0$.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/configuralimg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/configuralimg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/configuralimg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/configuralimg.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 9. Illustration of the Frankenstein test. </div> <p>We first compute the Pearson correlation score between configural sensitivity and hard image recognition ($-0.58$), hard caricature recognition ($-0.71$) shape bias on common objects ($-0.69$) and shape bias on novel objects ($-0.41$). Given that hard caricature recognition is the most highly correlated metric, we plot configural sensitivity against hard caricature recognition in Figure 10. We observe that the performance of current models is very weak, barely lower than $0$. Only large-scale weakly supervised models are significantly more sensitive than a BagNet-9, a model that cannot show configural sensitivity by design. All models are less sensitive than the estimated adults. We conclude that even shape-biased models likely rely on local shape cues instead of the global arrangement of parts.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/configural-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/configural-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/configural-1400.webp"/> <img src="/2025/assets/img/2025-04-28-toddlers-vs-vismodels/configural.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 10. Configural sensitivity of ML models against their shape bias on common objects. </div> <h1 id="conclusion">Conclusion</h1> <p>In this blog post, we have extended previous studies to investigate whether current ML models learn 4 fundamental visual properties that emerge in toddlers. While we did not observe major effects of the training setup (supervised, adversarial, self-supervised…), we found that cutting-edge models reach the estimated level of a toddler with respect to caricature recognition and the shape bias on common objects. For the shape bias on novel objects, the best models are close to toddlers. However, most of the considered models use bio-implausible data: a lot more diverse images, labels, millions of aligned language utterances, adversarial samples, etc… Models trained with egocentric visual images comparable to humans (VC-1, VIP, R3M) perform poorly on all benchmarks. In addition, other ML models still perform poorly on configural sensitivity and the proposed side-view bias compared to toddlers and adults, suggesting there is room for improvements. We cannot conclude on the performance of toddlers for configural sensitivity for the considered task. However, given the emerging configural sensitivity of toddlers <d-cite key="augustine2011parts"></d-cite> and the amplitude of the difference between models and adults, we presume that ML models are likely inferior to toddlers.</p> <p>We found that object recognition abilities positively correlate with caricature recognition, which also positively correlates with a strong shape bias. This suggests that these properties are connected in ML models. However, it does not mean that increasing the shape bias systematically leads to better recognition abilities, as evidenced by the Noisy-Student model (high shape bias, relatively low caricature recognition) and prior works <d-cite key="hermann2020origins"></d-cite>. Furthermore, showing a shape bias correlates with the side-view bias and the configural sensitivity, but it is far from enough to reach the level of humans. Thus, we speculate that the shape bias may be a first milestone of visual learning, on the way to generalizing objects based on the relative arrangement of parts.</p> <p>From a developmental perspective, prior work found that productive vocabulary size is a strong predictor of the shape bias in toddlers <d-cite key="gershkoff2004shape, jones2003late"></d-cite>. It could be that language aids in building visual representations or that strong visual representations are needed to produce language. Although we cannot conclude about a potential causal direction of this relation, we discovered that a shape bias similar to toddlers’ <em>can</em> emerge without language (DinoV2). Thus, one should not over-estimate the causal role of language in building visual representations.</p> <p>The origin and consequence of the side-view bias for toddlers is mostly unclear in the developmental literature. It may emerge because these views are very stable for small rotations <d-cite key="perrett1992use"></d-cite> or because they are standardized views for aligning views of objects <d-cite key="pereira2010early"></d-cite>. In both cases, preliminary experiments indicate this bias may boost their object recognition abilities <d-cite key="james2001manipulating"></d-cite>. We investigated for the first time the potential specificity of side-views for ML models and found that these views become more object-prototypical when the shape bias increases. This supports the hypothesis that toddlers’ bias emerges because these views are more prototypical. However, in this case, it remains unclear why side-views are still less object-prototypical than 3/4 views on average for ML models. It could be that ML models lack some visual property like configural sensitivity or that our dataset is too different from the set of 16 simple stimulus objects used in Pereira et al. (2010).</p> <p>This study presents several limitations. First, the experimental protocol does not perfectly follow either developmental experiments (often based on language) or machine learning protocols (often based on vision-language models or supervised labels). We chose not to rely on language to test a broad set of models beyond vision-language models, and we decided to not use supervised heads as it is unclear how it relates to toddlers’ visual representations. Thus, we assume that, during their task, toddlers perform a form of cognitive comparison that resembles our comparison of representations. Second, the set of stimuli generally varies between developmental studies and ours, allowing to only approximate how a toddler would perform. This is especially salient in our study of configural sensitivity: to the best of our knowledge, toddlers have not been tested on Frankenstein silhouettes. Third, the set of stimuli is small and often biased (very small for caricatures, only animals for configural sensitivity, white background in almost all datasets…). Despite that, our study provides a global picture of the presence and interplay of toddlers’ visual properties in ML models. We hope it will spur research aimed at addressing the gap between models and toddlers.</p> <h1 id="acknowledgments">Acknowledgments</h1> <p>This work was funded by the Deutsche Forschungsgemeinschaft (DFG project Abstract REpresentations in Neural Architectures (ARENA)), as well as the projects The Adaptive Mind and The Third Wave of Artificial Intelligence funded by the Excellence Program of the Hessian Ministry of Higher Education, Science, Research and Art (HMWK). JT was supported by the Johanna Quandt foundation. We gratefully acknowledge support from the Goethe-University (NHR Center NHR@SW) and Marburg University (MaRC3a) for providing computing and data-processing resources needed for this work.</p> <h1 id="models">Models</h1> <p>To reproduce and strengthen the claims reported papers and this blogpost, we test a very diverse set of pre-trained models, including those addressing specific shortcomings of ML models. We use a mix of convolutional architectures (CNN) and vision transformers (ViT).</p> <p>As most of previous experiments, we assess standard models based on random initializations and supervised ImageNet training for both ResNet50 and ViT-L. We also consider two different adversarially trained supervised models <d-cite key="salman2020adversarially"></d-cite>, as previous works argued that they share similar adversarial robustness properties as humans <d-cite key="guo2022adversarially"></d-cite>. Finally, we add three supervised BagNet models that extract representations of small image patches in the deepest layer (corresponding to $9\times9$, $17\times17$ and $33\times33$ pixels)<d-cite key="brendel2018approximating"></d-cite>; this allows to give hints on whether the local structure is enough to fulfill a given task.</p> <p>On the weakly supervised side, we consider a strong ViT-L/16 trained on a large-scale dataset of hashtags (SWAG) <d-cite key="singh2022revisiting"></d-cite>, four vision-language models based on the openai CLIP with ResNet50 and ViT-L/14<d-cite key=" radford2021learning"></d-cite> and two open CLIP models trained on LAION-5B <d-cite key=" schuhmann2022laionb,cherti2023reproducible,ilharco_gabriel_2021_5143773"></d-cite> with a ConvNet-XXLarge and a ViT-G/14; these vision-language models show classification errors similar to humans <d-cite key="geirhos2021partial"></d-cite> and are very robust at recognizing caricatures <d-cite key="sheybani2024modelvsbaby"></d-cite>.</p> <p>We further add two strong methods for out-of-distribution image classification <d-cite key="geirhos2021partial"></d-cite>, namely Noisy-Student <d-cite key="xie2020self"></d-cite> and SWSL <d-cite key="yalniz2019billion"></d-cite>.</p> <p>We include state-of-the-art self-supervised instance discrimination (ID) methods trained on ImageNet-1K, namely MoCoV2 <d-cite key="chen2020improved"></d-cite>, BYOL <d-cite key=" grill2020bootstrap"></d-cite> and DinoV2 as a mainstream foundation model<d-cite key=" oquab2024dinov2"></d-cite>. We further include models trained on MVImgNet <d-cite key=" yu2023mvimgnet"></d-cite> to be less sensitive to viewpoints (SimCLR-TT and AA-SimCLR, a slightly more viewpoint-sensitive variant) <d-cite key="aubret2024self"></d-cite>. We also add a Masked Auto-Encoder (MAE) trained on ImageNet with a ViT-L/16 <d-cite key=" he2022masked"></d-cite>, as MAEs keep more information than ID methods about the spatial location of features.</p> <p>Finally, we evaluate three vision models trained on robotic and egocentric data, namely VIP <d-cite key="mavip"></d-cite>, VC-1<d-cite key="majumdar2023we"></d-cite> and R3M <d-cite key="nairr3m"></d-cite>. Such training data is supposed to be relatively close to the visual experience of toddlers.</p>]]></content><author><name>Arthur Aubret</name></author><summary type="html"><![CDATA[Despite recent advances in artificial vision systems, humans are still more data-efficient at learning strong visual representations. Psychophysical experiments suggest that toddlers develop fundamental visual properties between the ages of one and three, which affect their perceptual system for the rest of their life. They begin to recognize impoverished variants of daily objects, pay more attention to the shape of an object to categorize it, prefer objects in specific orientations and progressively generalize over the configural arrangement of objects' parts. This post examines whether these four visual properties also emerge in off-the-shelf machine learning (ML) vision models. We reproduce and complement previous studies by comparing toddlers and a large set of diverse pre-trained vision models for each visual property. This way, we unveil the interplay between these visual properties and highlight the main differences between ML models and toddlers. Code is available at (https://github. com/Aubret/BabyML).]]></summary></entry></feed>